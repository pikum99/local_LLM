{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94cb0c8f-b6ec-42b5-9cf7-f7b917811a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drryu\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\drryu\\anaconda3\\envs\\text2\\lib\\site-packages\\accelerate\\utils\\modeling.py:805: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10/cuda/CUDAAllocatorConfig.h:30.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90742d301807463b9bb49b81b9810e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\drryu\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:698: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models, also known as neural language models or deep learning language models, are artificial intelligence systems that have been trained on massive amounts of text data to understand and generate human-like language. These models typically consist of complex neural networks with multiple layers, which enable them to learn the patterns, structures, and relationships within language.\n",
      "\n",
      "The primary objective of large language models is to capture the statistical dependencies in language, allowing them to generate coherent and contextually relevant responses to prompts, questions, or even entire paragraphs. They have become increasingly popular and powerful in recent years due to their ability to handle various natural language tasks such as language translation, text completion, summarization, and conversational AI.\n",
      "\n",
      "These models often require significant computational resources and training data, which has led to the development of large-scale pre-trained models like GPT-3 (Google's Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers). These models can be fine-tuned for specific tasks, demonstrating their versatility and adaptability in various industries and applications.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen1.5-7B-Chat-GPTQ-Int4\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-7B-Chat-GPTQ-Int4\")\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a9824ec-2d65-4788-be14-f05a0f461ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model. lang: ja\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5c0564c-1971-4437-84ce-eb54cf84ca2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大規模な言語モデルは、人工知能の分野で重要な発展を遂げたものであり、近年ますます注目を集めています。これらのモデルは、数百万乃至数兆個のパラメータを持ち、巨額のテキストデータを学習することで、自然言語の理解と生成の能力を高めます。\\n\\n大規模な言語モデルは主にディープラインアプローチによって作られることで、例えばBERT（Bidirectional Encoder Representations from Transformers）、GPT（Generative Pre-Training）やTuring-NLGなどの代表的なモデルがあります。これらのモデルは多様なタスクに適用され，例えば文法解析、意味推測、創造的な文章生成、翻訳、クイズ解答などを行います。\\n\\n大規模な言語モデルの強みは、その強力な統計学的知識と大規模なデータの意味的な理解が組み合わさることで、人間のような自然な会話や文章の生成が可能になったことです。しかし、学習データの偏りやセキュリティ問題、データーパフォーマンスの制限なども課題として存在します。'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e8d0e-9fee-4951-a97d-134714baa4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
